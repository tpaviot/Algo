{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31cd2b1-d221-4be0-a791-08b71ff1c42f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Algorithme d'encodage de texte pour Large Language Models : Byte Pair Encoding (BPE) et Tokenization\n",
    "\n",
    "Pour celles et ceux d'entre vous qui ne disposent pas des droits d'accès pour utiliser les bibliothèques tierces requises pour ce TP (tiktoken et transformers), un environnement en ligne est disponible à l'URL https://mybinder.org/v2/gh/tpaviot/binderenv/HEAD?filepath=\n",
    "\n",
    "Nous allons travailler à diviser un texte en briques de base connues sous le nom de \"vocabulaire\" et d'associer ainsi à une chapine de caractères une succession d'entiers. La tokenisation la plus élémentaire est celle consistant à associer à un mot l'ensemble des valeurs de la table ASCII (le vocabulaire contient $2^7=128$ briques de base) :\n",
    "```\n",
    "print([ord(c) for c in \"Salut\"])\n",
    "[83, 97, 108, 117, 116]\n",
    "```\n",
    "\n",
    "\n",
    "Mais ceci n'est pas suffisant pour travailler avec les grands modèles de langage type ChatGPT.\n",
    "\n",
    "Le **corpus** de texte est le jeu de données connu sous le nom de \"Tiny shakespeare\" accessible à l'url : https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "Ce TP a été construit à partir, entre autres, des ressources suivantes disponibles en ligne, que vous êtes invité.e.s à prendre le temps de consulter (en particulier les vidéos de Karpathy) :\n",
    "\n",
    "* La documenation HuggingFace (https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)\n",
    "\n",
    "* La chaîne vidéo YouTube d'Andrej Karpathy (https://www.youtube.com/@AndrejKarpathy/)\n",
    "\n",
    "Ce **TP comporte trois parties** :\n",
    "* dans une première partie, nous nous intéressons à un algorithme qui associe à chaque lettre un entier\n",
    "\n",
    "* dans une deuxième partie, nous expérimentons un algorithme qui découpe les mots en token de 2 caractères\n",
    "\n",
    "* dans une troisième partie, nous implémentons un algorithmes plus avancé appelé BPE qui permet d'encoder n'importe quelle chaine de caractères dans une liste d'entiers.\n",
    "\n",
    "Pour chacun de ces trois algorithmes, nous comparons :\n",
    "* la qualité, c'est-à-dire le nombre d'entiers requis pour encoder la chaîne. Plus ce nombre est petit, plus l'aglo est efficient\n",
    "\n",
    "* le temps de calcul nécessaire pour encoder/décoder, étant entendu que, dans le domaine des modèles de langage, les texte à encoder peuvent être de plus giga octets. Plus le temps est court, plus l'algorithme est efficient.\n",
    "\n",
    "Dans la suite, on appellera `token` un de ces motifs de base et `tokenization` le processus consistant à découper une chaîne de caractères en éléments de base disponibles dans un vocabulaire. On utilisera le terme *token* pour désigner sans distinction l'élément de base du vocabulaire ou l'entier associé, qui sont en bijection. L'`encoding` est le processus permettant de passer de la chaîne à la liste de tokens et donc d'entiers, le `decodage` le processus réciproque (passage d'une liste d'entiers à une chaîne de caractères)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1ed68-cf1b-4fb0-8c5b-c52c70cbdbe2",
   "metadata": {},
   "source": [
    "## Question 1 - Chargement du jeu de données\n",
    "\n",
    "Avec la commande `wget` directement dans ce notebook, télécharger le contenu du fichier `tinyshakespeare` et le stocker dans le répertoire courant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ddaa536-439e-4bcb-8080-a9c1c0a18246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-29 08:46:53--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Résolution de raw.githubusercontent.com (raw.githubusercontent.com)… 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8001::154, ...\n",
      "Connexion à raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 1115394 (1,1M) [text/plain]\n",
      "Enregistre : ‘input.txt.5’\n",
      "\n",
      "input.txt.5         100%[===================>]   1,06M  --.-KB/s    ds 0,08s   \n",
      "\n",
      "2023-11-29 08:46:54 (13,5 MB/s) - ‘input.txt.5’ enregistré [1115394/1115394]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e273e-6c31-450b-a61c-943e8c304b58",
   "metadata": {},
   "source": [
    "# Première partie - Character Level Tokenization\n",
    "\n",
    "## Question 2\n",
    "\n",
    "* Charger le contenu du fichier dans une variable nommée `text`. Vous spécifierez un encodage de type utf-8 ;\n",
    "\n",
    "* afficher les 200 premiers caractères du texte ;\n",
    "\n",
    "* afficher le nombre total de caractères du texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdb7ff22-25fc-4d82-8639-4eca14c81944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "Nombre de caractères : 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:200])\n",
    "print(f\"Nombre de caractères : {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa25f64a-6804-4d2d-9f48-666dc97836ea",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Créer une fonction `build_vocab` qui prend comme paramètre une chaîne de carcatères `input_str` et qui renvoie :\n",
    "\n",
    "* la liste  `chars` de tous les caractères utilisés, sans doublon, classée dans l'ordre des codes ASCII des caractères\n",
    "\n",
    "* la taille `vocab_size` de cette liste\n",
    "\n",
    "* vérifier avec un `assert` que pour la chaîne `\"Andrej Karpathy, né le 23 octobre 1986, est un informaticien slovaco-canadien qui a été directeur de l'intelligence artificielle et du pilotage automatique chez Tesla. Il travaille actuellement pour OpenAI\"` le résultat concaténé retourné est `\" ',-.123689AIKOTabcdefghijlmnopqrstuvyzé\"` et la longueur `40`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d69c2acc-b261-4e85-a736-71f2eca407c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(input_str):\n",
    "    chars = sorted(set(input_str))\n",
    "    vocab_size = len(chars)\n",
    "    return chars, vocab_size\n",
    "\n",
    "l, s = build_vocab(\"Andrej Karpathy, né le 23 octobre 1986, est un informaticien slovaco-canadien qui a été directeur de l'intelligence artificielle et du pilotage automatique chez Tesla. Il travaille actuellement pour OpenAI\")\n",
    "assert ''.join(l) == \" ',-.123689AIKOTabcdefghijlmnopqrstuvyzé\"\n",
    "assert s == 40\n",
    "\n",
    "chars, vocab_size = build_vocab(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19286932-7d2c-44dd-8585-58661951df14",
   "metadata": {},
   "source": [
    "## Question 4. Character Level Tokenization\n",
    "Il s'agit de convertir ce texte en une séquence d'entiers à partir du vocabulaire défini précédemment.\n",
    "\n",
    "* créer une fonction `encode` qui prend en paramètre une liste de caractères et renvoie la liste des indices des caractères correspondant dans la liste `vocab`\n",
    "\n",
    "* créer une fonction `decode` qui est la fonction réciproque\n",
    "\n",
    "* vérifier que `encode(\"hii there\")` renvoie `[46, 47, 47, 1, 58, 46, 43, 56, 43]`\n",
    "\n",
    "* vérifier que `decode([46, 47, 47, 1, 58, 46, 43, 56, 43])` renvoie `\"hii_there\"`\n",
    "\n",
    "Dans cette question, nous avons associé, dans la fonction `encode`, un entier à chaque caractère, ce qui s'appelle `Character Level Tokenization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d551465d-e257-42bf-a26d-d595492dad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(str_input):\n",
    "    return [chars.index(c) for c in str_input]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([chars[entier] for entier in l])\n",
    "\n",
    "assert encode('hii there') == [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
    "assert decode([46, 47, 47, 1, 58, 46, 43, 56, 43]) == 'hii there'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86050951-b96f-499d-aee5-2979bd7b20c5",
   "metadata": {},
   "source": [
    "## Question 5. Performances de notre Character Encoder\n",
    "\n",
    "* Mesurer le temps total nécessaire pour encoder le texte complet du texte tiny_shakespeare avec la fonction précédente ;\n",
    "\n",
    "* Afficher la vitesse d'encodage en octets/secondes ;\n",
    "\n",
    "* Mesurer et afficher le nombre d'éléments du texte encodé ;\n",
    "\n",
    "* Mesurer le temps total de décodage pour le texte, et exprimer de la même manière la vitesse de décodage en octets/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dc1c137-e2bc-4451-a87d-2973e69c35b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'éléments de la liste encodée: 1115394 entiers.\n",
      "Temps d'encodage en octets/seconde: 2662282\n",
      "Temps de décodage en octets/seconde: 30516042\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.perf_counter()\n",
    "shakespeare_encoded = encode(text)\n",
    "t2 = time.perf_counter()\n",
    "shakespeare_decoded = decode(shakespeare_encoded)\n",
    "t3 = time.perf_counter()\n",
    "print(f\"Nombre d'éléments de la liste encodée: {len(shakespeare_encoded)} entiers.\")\n",
    "print(f\"Temps d'encodage en octets/seconde: {int(len(shakespeare_encoded)/(t2-t1))}\")\n",
    "print(f\"Temps de décodage en octets/seconde: {int(len(shakespeare_decoded)/(t3-t2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab7918-42dc-4fa4-ae5a-0ad765c31921",
   "metadata": {},
   "source": [
    "## Question 6. Qualité de notre Character Encoder\n",
    "\n",
    "* encoder le texte suivant: \"Napoleon is a spectacle-filled action epic that details the checkered rise and fall of the iconic French Emperor Napoleon Bonaparte\". Quelle est la taille de la liste obtenue ?\n",
    "  \n",
    "* encoder le texte suivant: \"Napoléon est un film réalisé par Ridley Scott avec Joaquin Phoenix, Vanessa Kirby.\" Que constatez-vous ? quelle solution pouvez-vous apporter ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "239ace76-0c5b-40f0-9956-612a4c3a45f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'é' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nap_eng \u001b[38;5;241m=\u001b[39m encode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNapoleon is a spectacle-filled action epic that details the checkered rise and fall of the iconic French Emperor Napoleon Bonaparte\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(nap_eng))\n\u001b[0;32m----> 3\u001b[0m np_fr \u001b[38;5;241m=\u001b[39m \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNapoléon est un film réalisé par Ridley Scott avec Joaquin Phoenix, Vanessa Kirby.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(str_input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(str_input):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [chars\u001b[38;5;241m.\u001b[39mindex(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m str_input]\n",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(str_input):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mchars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m str_input]\n",
      "\u001b[0;31mValueError\u001b[0m: 'é' is not in list"
     ]
    }
   ],
   "source": [
    "nap_eng = encode(\"Napoleon is a spectacle-filled action epic that details the checkered rise and fall of the iconic French Emperor Napoleon Bonaparte\")\n",
    "print(len(nap_eng))\n",
    "np_fr = encode(\"Napoléon est un film réalisé par Ridley Scott avec Joaquin Phoenix, Vanessa Kirby.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62de907f-9ad2-4a94-8d1e-07c65f7756aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visiblement, une exception est levée du fait de la présence du caractère \"è\" qui n'est pas dans le vocabulaire.\n",
    "# Je propose d'étendre le jeu de données avec un texte en français comprenant des accents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6c44d-909e-4052-8a79-89d6a6afffdb",
   "metadata": {},
   "source": [
    "# Partie 2\n",
    "\n",
    "## Question 7. Introduction à la problématique de Subword tokenization\n",
    "Nous pouvons travailler à partir d'un vocabulaire qui n'est pas constitué que de caractères simples mais de séquences de 2 caractères, ce qui permettra d'avoir des encodages plus courts.\n",
    "\n",
    "Par exemple, si le vocabulaire est constitué des éléments: `vocab = ['ch', 'ien', 'at']` alors l'encodage du mot `chien` sera `[0, 1]` et celui du mot `chat` sera `[0, 2]`, ne prenant dans les deux cas que deux entiers alors qu'il en aurait fallu 4 avec la méthode des questions précédentes. Il s'agit dans ce cas d'un algorithme de \"SubWord encoding\", plus performant de toute évidence puisqu'il divise dans ce cas le nombre d'entiers par 2.\n",
    "\n",
    "* définir une fonction `split_pair` qui prend une chaine de caractères et scinde la chaîne de caractères en groupes de deux caractères. Si la longueur de la chaîne de caractères est impaire alors la dernière lettre sera un caractère seul.\n",
    "\n",
    "* vérifier que la fonction `split_pair` appliquée à la chaîne `\"Napoleon\"` renvoie `['Na', 'po', 'le', 'on']`\n",
    "\n",
    "* vérifier que la fonction `split_pair` appliquée à la chaîne `\"Napoleon3\"`renvoie `['Na', 'po', 'le', 'on', '3']`\n",
    "\n",
    "* comme dans la question 3, construire ensuite un vocabulaire à partir de cette liste de paires de caractères, sans doublons. Vérifier que pour la chaîne `\"un chien et un chat rigolent, ha ha\"` vous obtenez le vocabulaire :\n",
    "`[' c', ' e', ' h', ', ', 'a', 'en', 'go', 'ha', 'hi', 'le', 'nt', 'ri', 't ', 'un']`\n",
    "et une taille de vocabulaire de `14` éléments.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cb42201-58e1-49eb-a5f4-562b29747833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pair(input_str):\n",
    "    pair = True\n",
    "    if len(input_str) % 2 == 1: #impair\n",
    "        pair = False\n",
    "    if not pair:\n",
    "        last_character = input_str[-1]\n",
    "        input_str = input_str[0:len(input_str)-1]\n",
    "    splitted_pair = [input_str[i:i+2] for i in range(0, len(input_str), 2)]\n",
    "    if not pair:\n",
    "        splitted_pair += last_character\n",
    "    return splitted_pair\n",
    "\n",
    "# vérification\n",
    "assert split_pair(\"Napoleon\") == ['Na', 'po', 'le', 'on']\n",
    "assert split_pair(\"Napoleon3\") == ['Na', 'po', 'le', 'on', '3']\n",
    "\n",
    "def build_vocab_pair(input_str):\n",
    "    pairs = split_pair(input_str)\n",
    "    vocab = sorted(set(pairs))\n",
    "    vocab_size = len(vocab)\n",
    "    return vocab, vocab_size\n",
    "\n",
    "v, s = build_vocab_pair(\"un chien et un chat rigolent, ha ha\")\n",
    "assert v == [' c', ' e', ' h', ', ', 'a', 'en', 'go', 'ha', 'hi', 'le', 'nt', 'ri', 't ', 'un']\n",
    "assert s == 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea15e77-c584-4b2e-b75d-adb8f3fac420",
   "metadata": {},
   "source": [
    "## Question 8. Un pair encoder simplifié\n",
    "\n",
    "* créer le vocabulaire correspondant au jeu de données `tiny_shakespeare`. Vérifier que la taille du vocabulaire est de `1334` ;\n",
    "\n",
    "* créer une fonction `encode_pair` et `decode_pair` qui s'appuient sur le vocabulaire précédent ;\n",
    "\n",
    "* vérifier que  l'encoder renvoie pour la chaine `'I say unto you, what he hath done famously'` est `[391, 1165, 1296, 1237, 1208, 104, 1085, 156, 1267, 710, 88, 794, 887, 1203, 84, 1078, 794, 839, 1012, 1241, 993]`\n",
    "\n",
    "* vérifier que le décodage renvoie la bonne chaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bbf2cbe-4b61-4c5a-9c05-86edf7bac23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La taille du vocabulaire est: 1334\n"
     ]
    }
   ],
   "source": [
    "vocab, siz = build_vocab_pair(text)\n",
    "print(f\"La taille du vocabulaire est: {siz}\")\n",
    "\n",
    "def encode_pair(str_input):\n",
    "    lis = split_pair(str_input)\n",
    "    return [vocab.index(c) for c in lis]\n",
    "\n",
    "def decode_pair(l):\n",
    "    return ''.join([vocab[entier] for entier in l])\n",
    "\n",
    "assert encode_pair('I say unto you, what he hath done famously') == [391, 1165, 1296, 1237, 1208, 104, 1085, 156, 1267, 710, 88, 794, 887, 1203, 84, 1078, 794, 839, 1012, 1241, 993]\n",
    "assert decode_pair([391, 1165, 1296, 1237, 1208, 104, 1085, 156, 1267, 710, 88, 794, 887, 1203, 84, 1078, 794, 839, 1012, 1241, 993]) == 'I say unto you, what he hath done famously'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c91e1-c6cc-4249-9393-bf36a81032fe",
   "metadata": {},
   "source": [
    "## Question 9. Performances de ce pair encoder basique\n",
    "\n",
    "* reprendre les mêmes questions que la question 5 pour mesurer les performances de l'encoder et du decoder en octets/s pour le texte complet tiney_shakespeare.\n",
    "\n",
    "* conclure quant à la comparaison entre les deux encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a27180c9-c11e-42e8-9a66-c9c5af3bc511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'éléments de la liste encodée: 557697 entiers.\n",
      "Temps d'encodage en octets/secondes: 128282\n",
      "Temps de décodage en octets/secondes: 60039143\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.perf_counter()\n",
    "shakespear_encoded = encode_pair(text)\n",
    "t2 = time.perf_counter()\n",
    "shakespear_decoded = decode_pair(shakespear_encoded)\n",
    "t3 = time.perf_counter()\n",
    "print(f\"Nombre d'éléments de la liste encodée: {len(shakespear_encoded)} entiers.\")\n",
    "print(f\"Temps d'encodage en octets/secondes: {int(len(shakespear_encoded)/(t2-t1))}\")\n",
    "print(f\"Temps de décodage en octets/secondes: {int(len(shakespear_decoded)/(t3-t2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7618b4-91ba-4b59-a6d6-9db923256d3c",
   "metadata": {},
   "source": [
    "## Question 10. Limites de notre pair encoder\n",
    "\n",
    "* Encoder la chaine \"BUT Informatique de Nevers\" avec le pair encoder précédent.\n",
    "\n",
    "* Proposer et implémenter une solution pour corriger précédent pour le cas où les paires ne sont pas trouvées dans le vocabulaire.\n",
    "\n",
    "* L'encoder modifié devra permettre d'obtenir la même longueur pour le tiny_shakespeare encodé, et proposer une solution pour n'importe quelle chaîne de caractères pour les caractères présents dans le texte original tiny_shakespeare.\n",
    "\n",
    "* Vérifier l'impact de votre modification en termes de performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67dc4309-1a26-4451-a67a-f641be96d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Une paire n'est pas trouvée. Erreur : 'T ' is not in list\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    encode_pair(\"BUT Informatique de Nevers\")\n",
    "except ValueError as e:\n",
    "    print(\"Une paire n'est pas trouvée. Erreur :\", e)\n",
    "# cela ne fonctionne pas, car la paire \"T \" n'est pas dans le texte d'origine.\n",
    "\n",
    "# solution proposée: si une paire n'est pas trouvée, je la coupe en deux et j'encode chaque caractère.\n",
    "# mon vocabulaire est donc la fusion des deux vocabulaires précédents : celui par caractères, et celui par paires.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eaba4c-ebb5-4daf-91fc-2795bad681d4",
   "metadata": {},
   "source": [
    "## Partie 3. Algorithme Byte Pair Encoding (BPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e892ba-c8d2-4910-b10c-8be1e3cba3ea",
   "metadata": {},
   "source": [
    "Dans l'exemple précédent, nous avons construit des paires de lettres de manière irréfléchie, simplement en stockant les paires au fur et à mesure qu'elles se présentent. L'algorithme Byte Pair Encoding permet de construire un vocabulaire de **token** (groupes de 2 ou plus lettres formant le vocabulaire de base) à partir de l'**analyse de la fréquence d'occurrence** dans un **corpus** (dans notre cas, le corpus est le fichier \"tiny shakespeare\"). Byte Pair Encoding (BPE) est un des algorithmes de **tokenization** les plus populaires, utilisé notamment dans les grands modèles de langage type ChatGPT.\n",
    "\n",
    "Nous allons, dans les questions suivantes, implémenter un algorithme BPE à partir de zéro, puis ensuite nous le confronterons à des implémentations industrielles libres (celles d'OpenAI et HuggingFace)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de5fa8-9dc5-4cd0-8396-6c5a1d3fa51c",
   "metadata": {},
   "source": [
    "## Question 11. Fréquence de mots\n",
    "\n",
    "Ecrire une fonction `frequence_mots` qui prend comme paramètre une chaine de caractères `input_str` et qui renvoie un dictionnaire dont les clés sont les mots et les valeurs sont le nombre d'occurrences de ces mots dans la chaîne.\n",
    "\n",
    "Par exemple, dans la chaîne \"le chien Pluto et le chien Milou\", le mot \"chien\" est présent 2 fois, le mot 'le' aussi, on obtiendra donc: \n",
    "\n",
    "{'le': 2, 'chien': 2, 'Pluto': 1, 'et': 1, 'Milou': 1)}\n",
    "\n",
    "Nous allons travailler, dans ce qui suit, avec la chaîne de caractères suivante:\n",
    "```python\n",
    "corpus = \"This is the Hugging Face Course. This chapter is about tokenization. This section shows several tokenizer algorithms. Hopefully, you will be able to understand how they are trained and generate tokens.\"\n",
    "```\n",
    "\n",
    "Vérifier que\n",
    "```python\n",
    "mots_freqs = frequence_mots(corpus)\n",
    "print(mots_freqs)\n",
    "```\n",
    "\n",
    "renvoie bien\n",
    "\n",
    "```python\n",
    "{'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course.': 1, 'chapter': 1, 'about': 1, 'tokenization.': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms.': 1, 'Hopefully,': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens.': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbead76c-e064-4baa-b5d9-5cd49d414822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course.': 1, 'chapter': 1, 'about': 1, 'tokenization.': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms.': 1, 'Hopefully,': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens.': 1}\n"
     ]
    }
   ],
   "source": [
    "def frequence_mots(input_str):\n",
    "    # renvoie une liste de mots\n",
    "    mots = input_str.split()\n",
    "\n",
    "    freqs = {}\n",
    "    for mot in mots:\n",
    "        # on compte le nombre d'occurrences\n",
    "        freqs[mot] = mots.count(mot)\n",
    "    return freqs\n",
    "\n",
    "corpus = \"This is the Hugging Face Course. This chapter is about tokenization. This section shows several tokenizer algorithms. Hopefully, you will be able to understand how they are trained and generate tokens.\"\n",
    "mots_freqs = frequence_mots(corpus)\n",
    "print(mots_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaaa44c-8d77-4be1-b21e-3e44da6f3e3c",
   "metadata": {},
   "source": [
    "## Question 12. Alphabet et vocabulaire\n",
    "\n",
    "L'étape suivante est de déterminer le vocabulaire de base, formé par l'ensemble des caractères utilisés dans le corpus.\n",
    "\n",
    "* Ecrire une fonction `calcule_alphabet` qui prend comme paramètre un dictionnaire de fréquences de mots `dict_freq` et qui renvoie la liste des lettres utilisées ;\n",
    "\n",
    "* ensuite, créer le `vocabulaire` en ajoutant le token spécial `<|endoftext|>`:\n",
    "```python\n",
    "vocabulaire = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "```\n",
    "\n",
    "Vous vérifierez que vous obtenez le vocabulaire suivant :\n",
    "```python\n",
    "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f03f0c3-3d36-4cab-bd70-4613445fb9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "def calcule_alphabet(dict_freq):\n",
    "    alphabet = []\n",
    "\n",
    "    for word in dict_freq.keys():\n",
    "        for letter in word:\n",
    "            if letter not in alphabet:\n",
    "                alphabet.append(letter)\n",
    "    alphabet.sort()\n",
    "    return alphabet\n",
    "\n",
    "alphabet = calcule_alphabet(mots_freqs)\n",
    "vocabulaire = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "print(vocabulaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ea284-e593-4172-b3ea-fbd7ddaf8565",
   "metadata": {},
   "source": [
    "## Question 13. Splits\n",
    "\n",
    "Ecrire une fonction `calcule_splits` qui prend comme paramètre une liste de mots `liste_mots`et qui renvoie un dictionnaire qui associe à chaque mot la liste des lettres qui le composent. Par exemple :\n",
    "```python\n",
    "{'This': ['T', 'h', 'i', 's'], 'is': ['i', 's'], ...\n",
    "```\n",
    "Appliquer cette fonction aux mots servant de clé dans la dictionnaire `freqs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62272ef6-77d3-4c1a-92d0-988d52c2709d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ['T', 'h', 'i', 's'], 'is': ['i', 's'], 'the': ['t', 'h', 'e'], 'Hugging': ['H', 'u', 'g', 'g', 'i', 'n', 'g'], 'Face': ['F', 'a', 'c', 'e'], 'Course.': ['C', 'o', 'u', 'r', 's', 'e', '.'], 'chapter': ['c', 'h', 'a', 'p', 't', 'e', 'r'], 'about': ['a', 'b', 'o', 'u', 't'], 'tokenization.': ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.'], 'section': ['s', 'e', 'c', 't', 'i', 'o', 'n'], 'shows': ['s', 'h', 'o', 'w', 's'], 'several': ['s', 'e', 'v', 'e', 'r', 'a', 'l'], 'tokenizer': ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'], 'algorithms.': ['a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's', '.'], 'Hopefully,': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y', ','], 'you': ['y', 'o', 'u'], 'will': ['w', 'i', 'l', 'l'], 'be': ['b', 'e'], 'able': ['a', 'b', 'l', 'e'], 'to': ['t', 'o'], 'understand': ['u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'], 'how': ['h', 'o', 'w'], 'they': ['t', 'h', 'e', 'y'], 'are': ['a', 'r', 'e'], 'trained': ['t', 'r', 'a', 'i', 'n', 'e', 'd'], 'and': ['a', 'n', 'd'], 'generate': ['g', 'e', 'n', 'e', 'r', 'a', 't', 'e'], 'tokens.': ['t', 'o', 'k', 'e', 'n', 's', '.']}\n"
     ]
    }
   ],
   "source": [
    "def calcule_splits(liste_mots):\n",
    "    d = {}\n",
    "    for mot in liste_mots:\n",
    "        d[mot] = list(mot)\n",
    "    return d\n",
    "\n",
    "splits = calcule_splits(mots_freqs.keys())\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0121c-385d-4933-8243-6a95d8712e24",
   "metadata": {},
   "source": [
    "## Question 14 - Fréquence de paires de lettres ou groupes de lettres\n",
    "\n",
    "Il s'agit ensuite de déterminer la fréquence de l'occurrence de chaque paire de lettres. Par exemple, le mot 'This' est associé aux lettres 'T', 'h', 'i et 's'. Il faut chercher, dans tous les mots, le nombre d'occurrences de 'T', 'h', puis de 'h','i', et de 'i','s'. Et ainsi de suite pour chaque mot.\n",
    "\n",
    "Ecrire une fonction `calcule_pair_freqs` qui retourne un dictionnaire avec comme clé un couple de lettres et comme valeur le nombre d'occurrences trouvées dans tous les mots. \n",
    "\n",
    "On aura par exemple en sortie:\n",
    "\n",
    "```python\n",
    "{('T', 'h'): 3,\n",
    " ('h', 'i'): 3,\n",
    " ('i', 's'): 5,\n",
    " ('t', 'h'): 3,\n",
    " ('h', 'e'): 2,\n",
    " ('H', 'u'): 1,\n",
    " ('u', 'g'): 1,\n",
    " ('g', 'g'): 1,\n",
    " ('g', 'i'): 1,\n",
    " ('i', 'n'): 2,\n",
    " ('n', 'g'): 1,\n",
    " ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75ffee04-49d5-481e-b10d-46b93870484f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('T', 'h'): 3, ('h', 'i'): 3, ('i', 's'): 5, ('t', 'h'): 3, ('h', 'e'): 2, ('H', 'u'): 1, ('u', 'g'): 1, ('g', 'g'): 1, ('g', 'i'): 1, ('i', 'n'): 2, ('n', 'g'): 1, ('F', 'a'): 1, ('a', 'c'): 1, ('c', 'e'): 1, ('C', 'o'): 1, ('o', 'u'): 3, ('u', 'r'): 1, ('r', 's'): 2, ('s', 'e'): 3, ('e', '.'): 1, ('c', 'h'): 1, ('h', 'a'): 1, ('a', 'p'): 1, ('p', 't'): 1, ('t', 'e'): 2, ('e', 'r'): 5, ('a', 'b'): 2, ('b', 'o'): 1, ('u', 't'): 1, ('t', 'o'): 4, ('o', 'k'): 3, ('k', 'e'): 3, ('e', 'n'): 4, ('n', 'i'): 2, ('i', 'z'): 2, ('z', 'a'): 1, ('a', 't'): 2, ('t', 'i'): 2, ('i', 'o'): 2, ('o', 'n'): 2, ('n', '.'): 1, ('e', 'c'): 1, ('c', 't'): 1, ('s', 'h'): 1, ('h', 'o'): 2, ('o', 'w'): 2, ('w', 's'): 1, ('e', 'v'): 1, ('v', 'e'): 1, ('r', 'a'): 3, ('a', 'l'): 2, ('z', 'e'): 1, ('l', 'g'): 1, ('g', 'o'): 1, ('o', 'r'): 1, ('r', 'i'): 1, ('i', 't'): 1, ('h', 'm'): 1, ('m', 's'): 1, ('s', '.'): 2, ('H', 'o'): 1, ('o', 'p'): 1, ('p', 'e'): 1, ('e', 'f'): 1, ('f', 'u'): 1, ('u', 'l'): 1, ('l', 'l'): 2, ('l', 'y'): 1, ('y', ','): 1, ('y', 'o'): 1, ('w', 'i'): 1, ('i', 'l'): 1, ('b', 'e'): 1, ('b', 'l'): 1, ('l', 'e'): 1, ('u', 'n'): 1, ('n', 'd'): 3, ('d', 'e'): 1, ('s', 't'): 1, ('t', 'a'): 1, ('a', 'n'): 2, ('e', 'y'): 1, ('a', 'r'): 1, ('r', 'e'): 1, ('t', 'r'): 1, ('a', 'i'): 1, ('n', 'e'): 2, ('e', 'd'): 1, ('g', 'e'): 1, ('n', 's'): 1}\n"
     ]
    }
   ],
   "source": [
    "def calcule_pair_freqs(splits, word_freqs):\n",
    "    pair_freqs = {}\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            if pair in pair_freqs:\n",
    "                pair_freqs[pair] += freq\n",
    "            else:\n",
    "                pair_freqs[pair] = freq  # la première fois qu'on la trouve\n",
    "    return pair_freqs\n",
    "\n",
    "pair_freqs = calcule_pair_freqs(splits, mots_freqs)\n",
    "print(pair_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3a797-8b5e-47e7-a0c7-8d9451d5301a",
   "metadata": {},
   "source": [
    "## Question 15. Paire la plus fréquente\n",
    "\n",
    "Créer une fonction `paire_la_plus_frequente` qui prend comme paramètre le dictionnaire issu de la fonction précédente (celui contenant la fréquence de chaque paire) et qui retourne la paire la plus fréquente du corpus ainsi que le nombre correspondant à la fréquence. Si deux paires présentent le même nombre d'occurrences, la fonction renvoie la première paire rencontrée dans le parcours de l'ensemble des paires.\n",
    "\n",
    "Vérifier que `paire_la_plus_frequente(pair_freqs)` renvoie `(('i', 's'), 5)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b6077c5-8bd5-49a8-a01d-65b8072cea6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('i', 's'), 5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def paire_la_plus_frequente(pair_freq_dict):\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    \n",
    "    for pair, freq in pair_freq_dict.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    \n",
    "    return best_pair, max_freq\n",
    "\n",
    "paire_la_plus_frequente(pair_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd64286-6c3c-44b8-9bbf-2e53b74b4f5b",
   "metadata": {},
   "source": [
    "## Question 16. Extension du vocabulaire\n",
    "\n",
    "On va maintenant rajouter au vocabulaire les combinaisons de lettres les plus fréquentes dans le texte.\n",
    "\n",
    "* créer un dictionnaire `fusions` qui associe, à la paire précédente `('i', 's')` la paire concaténée `'is'`\n",
    "\n",
    "* ajouter cette chaîne concaténée à la liste `vocabulaire`.\n",
    "\n",
    "Remarque : cette question est très facile, inutile de créer une fonction.\n",
    "\n",
    "Vous vérifierez que\n",
    "\n",
    "```python\n",
    "print(fusions)\n",
    "print(vocabulaire)\n",
    "```\n",
    "renvoie\n",
    "```\n",
    "{('i', 's'): 'is'}\n",
    "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'is']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10b91e46-4718-40df-b4aa-febbb8e5d541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('i', 's'): 'is'}\n",
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'is']\n"
     ]
    }
   ],
   "source": [
    "fusions = {(\"i\", \"s\"): \"is\"}\n",
    "vocabulaire.append(\"is\")\n",
    "print(fusions)\n",
    "print(vocabulaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb801b-d118-489b-898b-30ba79e73fe3",
   "metadata": {},
   "source": [
    "## Question 16. Fusion des paires\n",
    "\n",
    "C'est la dernière étape de l'algorithme de création du vocabulaire. On se donne une taille maximale du vocabulaire `vocab_size` que l'on fixe à `50`. Reproduire l'étape précédente (reherche de la paire la plus fréquence, fusion, ajout au vocabulaire) jusqu'à ce que la taille maximale du vocabulaire soit atteinte.\n",
    "\n",
    "Pour cette valeur de `vocab_size`, vérifier que vous obtenez le vocabulaire suivant :\n",
    "\n",
    "```python\n",
    "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'is', 'is', 'er', 'to', 'en', 'Th', 'This', 'th', 'ou', 'se', 'tok', 'token', 'nd', 'the', 'in', 'ab', 'tokeni', 'tokeniz', 'at', 'io']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9900bfce-772d-41ea-94de-a4ec05cdbdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'is', 'is', 'er', 'to', 'en', 'Th', 'This', 'th', 'ou', 'se', 'tok', 'token', 'nd', 'the', 'in', 'ab', 'tokeni', 'tokeniz', 'at', 'io']\n"
     ]
    }
   ],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for mot in mots_freqs:\n",
    "        split = splits[mot]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[mot] = split\n",
    "    return splits\n",
    "\n",
    "vocab_size = 50\n",
    "\n",
    "while len(vocabulaire) < vocab_size:\n",
    "    pair_freqs = calcule_pair_freqs(splits, mots_freqs)\n",
    "    best_pair, max_freq = paire_la_plus_frequente(pair_freqs)\n",
    "    splits = merge_pair(best_pair[0], best_pair[1], splits)\n",
    "    fusions[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocabulaire.append(best_pair[0] + best_pair[1])\n",
    "\n",
    "print(vocabulaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f54eb-bc03-4e98-bca3-080a5c8fc347",
   "metadata": {},
   "source": [
    "## Question 17 - Tokenization\n",
    "\n",
    "La dernière étape de ce voyage vers les tokens consiste à créer une fonction `tokenize` qui prend une chaine de caractères et, comme dans le début de ce TP, contient l'ensemble des entiers faisant référence au vocabulaire de 50 termes construit précédemment.\n",
    "\n",
    "Vérifier que la tokenization :\n",
    "\n",
    "```python\n",
    "print(tokenize('This is not a token.'))\n",
    "```\n",
    "\n",
    "Donne bien\n",
    "\n",
    "```python\n",
    "['This', 'is', 'n', 'o', 't', 'a', 'token', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04f2625f-094f-4cf9-bcb4-f5ead7a8f4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'n', 'o', 't', 'a', 'token', '.']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    # première étape : découpage du mot en liste\n",
    "    splits = [list(word) for word in text.split()]\n",
    "    \n",
    "    for pair, merge in fusions.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])\n",
    "\n",
    "print(tokenize('This is not a token.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c50540-7976-483a-93e9-9965fea46600",
   "metadata": {},
   "source": [
    "## Question 18 - Bilan de la qualité algorithmique\n",
    "\n",
    "Pour la châine de caractères `This is not a token`, comparer la taille de la liste d'entiers obtenue pour chacun des 3 tokenizers étudiés.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da73d1-9e8e-4e3e-a09a-9b06c457478c",
   "metadata": {},
   "source": [
    "## Question 19 - Performances de notre BPE\n",
    "\n",
    "Reprendre les questions précédentes en travaillant à partir du corpus `tinyshakespeare`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e647ec-3ad1-4bba-8218-aada2429221c",
   "metadata": {},
   "source": [
    "## Question 20. Implémentations industrielles de tokenizers GPT2 - OpenAI, HuggingFace\n",
    "\n",
    "* BPE est utilisé par OpenAI pour ses gpt depuis gpt2. C'est la bibliothèque `tiktoken` (https://github.com/openai/tiktoken) qui implémente cet algorithme\n",
    "\n",
    "* BPE est aussi utilisé par un autre grand acteur de l'IA générative : HuggingFace, dans sa bibliothèque `transformers` (https://github.com/huggingface/transformers)\n",
    "\n",
    "Pour utiliser la bibliothèque **tiktoken**, encoder/décoder :\n",
    "```python\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(f\"Nombre d'élements dans le vocabulaire : {enc.n_vocab}\")\n",
    "enc.encode('This is not a token')\n",
    "```\n",
    "\n",
    "Pour utiliser la bibliothèque **transformers**, encoder/décoder :\n",
    "```python\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer(\"This is not a token\")['input_ids']\n",
    "```\n",
    "\n",
    "* Quelle est la taille du vocabulaire `gpt2`?\n",
    "\n",
    "* Vérifier que les deux implémentations renvoient la même liste d'entiers pour l'encodage de la chaîne `This is not a token`.\n",
    "\n",
    "* Comparer ces implémentations de BPE avec celle que nous avons faite précédemment.\n",
    "\n",
    "* Comparer ces deux bibliothèques en encodage/décodage par rapport à la vitesse en octets/seconde.\n",
    "\n",
    "* Conclure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "801cf09c-ecc5-4d0b-a9fb-3facc42079a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'élements dans le vocabulaire : 50257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1212, 318, 407, 257, 11241]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(f\"Nombre d'élements dans le vocabulaire : {enc.n_vocab}\")\n",
    "enc.encode(\"This is not a token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8669485-e613-4f4c-a4f9-36edd1cf9805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 407, 257, 11241]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer(\"This is not a token\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "adb94557-3395-448e-8fef-7ee74a16cf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps total pour encodage avec tiktoken en secondes : 0.13s\n",
      "Temps total pour encodage avec transformers en secondes: 0.59s\n",
      "Temps total pour décodage avec tiktoken en secondes : 0.01s\n",
      "Temps total pour décodage avec transformers en secondes: 1.13s\n"
     ]
    }
   ],
   "source": [
    "# comparaison des performances en encodage\n",
    "t0 = time.perf_counter()\n",
    "openai_encoded = enc.encode(text)  # encodage de tinyshakespeare avec OpenAI tiktoken\n",
    "t1 = time.perf_counter()\n",
    "hf_encoded = tokenizer(text)['input_ids']  # tokenization avec HuggingFace transformers\n",
    "t2 = time.perf_counter()\n",
    "#assert openai_encoded == hf_encoded['input_ids']  # on vérifie que le résultat est le même\n",
    "# affichage des performances comparées\n",
    "print(f\"Temps total pour encodage avec tiktoken en secondes : {t1-t0:.2f}s\")\n",
    "print(f\"Temps total pour encodage avec transformers en secondes: {t2-t1:.2f}s\")\n",
    "\n",
    "# et pour le décodage\n",
    "t0_bis = time.perf_counter()\n",
    "openai_decoded = enc.decode(openai_encoded)  # encodage de tinyshakespeare avec OpenAI tiktoken\n",
    "t1_bis = time.perf_counter()\n",
    "hf_decoded = tokenizer.decode(hf_encoded)\n",
    "t2_bis = time.perf_counter()\n",
    "\n",
    "print(f\"Temps total pour décodage avec tiktoken en secondes : {t1_bis-t0_bis:.2f}s\")\n",
    "print(f\"Temps total pour décodage avec transformers en secondes: {t2_bis-t1_bis:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
