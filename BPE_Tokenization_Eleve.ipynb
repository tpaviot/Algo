{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31cd2b1-d221-4be0-a791-08b71ff1c42f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Algorithme d'encodage de texte pour Large Language Models : Byte Pair Encoding (BPE) et Tokenization\n",
    "\n",
    "Pour celles et ceux d'entre vous qui ne disposent pas des droits d'accès pour utiliser les bibliothèques tierces requises pour ce TP (tiktoken et transformers), un environnement en ligne est disponible à l'URL https://mybinder.org/v2/gh/tpaviot/binderenv/HEAD?filepath=\n",
    "\n",
    "Nous allons travailler à diviser un texte en briques de base connues sous le nom de \"vocabulaire\" et d'associer ainsi à une chapine de caractères une succession d'entiers. La tokenisation la plus élémentaire est celle consistant à associer à un mot l'ensemble des valeurs de la table ASCII (le vocabulaire contient $2^7=128$ briques de base) :\n",
    "```\n",
    "print([ord(c) for c in \"Salut\"])\n",
    "[83, 97, 108, 117, 116]\n",
    "```\n",
    "\n",
    "\n",
    "Mais ceci n'est pas suffisant pour travailler avec les grands modèles de langage type ChatGPT.\n",
    "\n",
    "Le **corpus** de texte est le jeu de données connu sous le nom de \"Tiny shakespeare\" accessible à l'url : https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "Ce TP a été construit à partir, entre autres, des ressources suivantes disponibles en ligne, que vous êtes invité.e.s à prendre le temps de consulter (en particulier les vidéos de Karpathy) :\n",
    "\n",
    "* La documenation HuggingFace (https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)\n",
    "\n",
    "* La chaîne vidéo YouTube d'Andrej Karpathy (https://www.youtube.com/@AndrejKarpathy/)\n",
    "\n",
    "Ce **TP comporte trois parties** :\n",
    "* dans une première partie, nous nous intéressons à un algorithme qui associe à chaque lettre un entier\n",
    "\n",
    "* dans une deuxième partie, nous expérimentons un algorithme qui découpe les mots en token de 2 caractères\n",
    "\n",
    "* dans une troisième partie, nous implémentons un algorithmes plus avancé appelé BPE qui permet d'encoder n'importe quelle chaine de caractères dans une liste d'entiers.\n",
    "\n",
    "Pour chacun de ces trois algorithmes, nous comparons :\n",
    "* la qualité, c'est-à-dire le nombre d'entiers requis pour encoder la chaîne. Plus ce nombre est petit, plus l'aglo est efficient\n",
    "\n",
    "* le temps de calcul nécessaire pour encoder/décoder, étant entendu que, dans le domaine des modèles de langage, les texte à encoder peuvent être de plus giga octets. Plus le temps est court, plus l'algorithme est efficient.\n",
    "\n",
    "Dans la suite, on appellera `token` un de ces motifs de base et `tokenization` le processus consistant à découper une chaîne de caractères en éléments de base disponibles dans un vocabulaire. On utilisera le terme *token* pour désigner sans distinction l'élément de base du vocabulaire ou l'entier associé, qui sont en bijection. L'`encoding` est le processus permettant de passer de la chaîne à la liste de tokens et donc d'entiers, le `decodage` le processus réciproque (passage d'une liste d'entiers à une chaîne de caractères)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1ed68-cf1b-4fb0-8c5b-c52c70cbdbe2",
   "metadata": {},
   "source": [
    "## Question 1 - Chargement du jeu de données\n",
    "\n",
    "Avec la commande `wget` directement dans ce notebook, télécharger le contenu du fichier `tinyshakespeare` et le stocker dans le répertoire courant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddaa536-439e-4bcb-8080-a9c1c0a18246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e273e-6c31-450b-a61c-943e8c304b58",
   "metadata": {},
   "source": [
    "# Première partie - Character Level Tokenization\n",
    "\n",
    "## Question 2\n",
    "\n",
    "* Charger le contenu du fichier dans une variable nommée `text`. Vous spécifierez un encodage de type utf-8 ;\n",
    "\n",
    "* afficher les 200 premiers caractères du texte ;\n",
    "\n",
    "* afficher le nombre total de caractères du texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7ff22-25fc-4d82-8639-4eca14c81944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa25f64a-6804-4d2d-9f48-666dc97836ea",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Créer une fonction `build_vocab` qui prend comme paramètre une chaîne de carcatères `input_str` et qui renvoie :\n",
    "\n",
    "* la liste  `chars` de tous les caractères utilisés, sans doublon, classée dans l'ordre des codes ASCII des caractères\n",
    "\n",
    "* la taille `vocab_size` de cette liste\n",
    "\n",
    "* vérifier avec un `assert` que pour la chaîne `\"Andrej Karpathy, né le 23 octobre 1986, est un informaticien slovaco-canadien qui a été directeur de l'intelligence artificielle et du pilotage automatique chez Tesla. Il travaille actuellement pour OpenAI\"` le résultat concaténé retourné est `\" ',-.123689AIKOTabcdefghijlmnopqrstuvyzé\"` et la longueur `40`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c2acc-b261-4e85-a736-71f2eca407c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19286932-7d2c-44dd-8585-58661951df14",
   "metadata": {},
   "source": [
    "## Question 4. Character Level Tokenization\n",
    "Il s'agit de convertir ce texte en une séquence d'entiers à partir du vocabulaire défini précédemment.\n",
    "\n",
    "* créer une fonction `encode` qui prend en paramètre une liste de caractères et renvoie la liste des indices des caractères correspondant dans la liste `vocab`\n",
    "\n",
    "* créer une fonction `decode` qui est la fonction réciproque\n",
    "\n",
    "* vérifier que `encode(\"hii there\")` renvoie `[46, 47, 47, 1, 58, 46, 43, 56, 43]`\n",
    "\n",
    "* vérifier que `decode([46, 47, 47, 1, 58, 46, 43, 56, 43])` renvoie `\"hii_there\"`\n",
    "\n",
    "Dans cette question, nous avons associé, dans la fonction `encode`, un entier à chaque caractère, ce qui s'appelle `Character Level Tokenization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551465d-e257-42bf-a26d-d595492dad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86050951-b96f-499d-aee5-2979bd7b20c5",
   "metadata": {},
   "source": [
    "## Question 5. Performances de notre Character Encoder\n",
    "\n",
    "* Mesurer le temps total nécessaire pour encoder le texte complet du texte tiny_shakespeare avec la fonction précédente ;\n",
    "\n",
    "* Afficher la vitesse d'encodage en octets/secondes ;\n",
    "\n",
    "* Mesurer et afficher le nombre d'éléments du texte encodé ;\n",
    "\n",
    "* Mesurer le temps total de décodage pour le texte, et exprimer de la même manière la vitesse de décodage en octets/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1c137-e2bc-4451-a87d-2973e69c35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab7918-42dc-4fa4-ae5a-0ad765c31921",
   "metadata": {},
   "source": [
    "## Question 6. Qualité de notre Character Encoder\n",
    "\n",
    "* encoder le texte suivant: \"Napoleon is a spectacle-filled action epic that details the checkered rise and fall of the iconic French Emperor Napoleon Bonaparte\". Quelle est la taille de la liste obtenue ?\n",
    "  \n",
    "* encoder le texte suivant: \"Napoléon est un film réalisé par Ridley Scott avec Joaquin Phoenix, Vanessa Kirby.\" Que constatez-vous ? quelle solution pouvez-vous apporter ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239ace76-0c5b-40f0-9956-612a4c3a45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6c44d-909e-4052-8a79-89d6a6afffdb",
   "metadata": {},
   "source": [
    "# Partie 2\n",
    "\n",
    "## Question 7. Introduction à la problématique de Subword tokenization\n",
    "Nous pouvons travailler à partir d'un vocabulaire qui n'est pas constitué que de caractères simples mais de séquences de 2 caractères, ce qui permettra d'avoir des encodages plus courts.\n",
    "\n",
    "Par exemple, si le vocabulaire est constitué des éléments: `vocab = ['ch', 'ien', 'at']` alors l'encodage du mot `chien` sera `[0, 1]` et celui du mot `chat` sera `[0, 2]`, ne prenant dans les deux cas que deux entiers alors qu'il en aurait fallu 4 avec la méthode des questions précédentes. Il s'agit dans ce cas d'un algorithme de \"SubWord encoding\", plus performant de toute évidence puisqu'il divise dans ce cas le nombre d'entiers par 2.\n",
    "\n",
    "* définir une fonction `split_pair` qui prend une chaine de caractères et scinde la chaîne de caractères en groupes de deux caractères. Si la longueur de la chaîne de caractères est impaire alors la dernière lettre sera un caractère seul.\n",
    "\n",
    "* vérifier que la fonction `split_pair` appliquée à la chaîne `\"Napoleon\"` renvoie `['Na', 'po', 'le', 'on']`\n",
    "\n",
    "* vérifier que la fonction `split_pair` appliquée à la chaîne `\"Napoleon3\"`renvoie `['Na', 'po', 'le', 'on', '3']`\n",
    "\n",
    "* comme dans la question 3, construire ensuite un vocabulaire à partir de cette liste de paires de caractères, sans doublons. Vérifier que pour la chaîne `\"un chien et un chat rigolent, ha ha\"` vous obtenez le vocabulaire :\n",
    "`[' c', ' e', ' h', ', ', 'a', 'en', 'go', 'ha', 'hi', 'le', 'nt', 'ri', 't ', 'un']`\n",
    "et une taille de vocabulaire de `14` éléments.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb42201-58e1-49eb-a5f4-562b29747833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea15e77-c584-4b2e-b75d-adb8f3fac420",
   "metadata": {},
   "source": [
    "## Question 8. Un pair encoder simplifié\n",
    "\n",
    "* créer le vocabulaire correspondant au jeu de données `tiny_shakespeare`. Vérifier que la taille du vocabulaire est de `1334` ;\n",
    "\n",
    "* créer une fonction `encode_pair` et `decode_pair` qui s'appuient sur le vocabulaire précédent ;\n",
    "\n",
    "* vérifier que  l'encoder renvoie pour la chaine `'I say unto you, what he hath done famously'` est `[391, 1165, 1296, 1237, 1208, 104, 1085, 156, 1267, 710, 88, 794, 887, 1203, 84, 1078, 794, 839, 1012, 1241, 993]`\n",
    "\n",
    "* vérifier que le décodage renvoie la bonne chaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf2cbe-4b61-4c5a-9c05-86edf7bac23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c91e1-c6cc-4249-9393-bf36a81032fe",
   "metadata": {},
   "source": [
    "## Question 9. Performances de ce pair encoder basique\n",
    "\n",
    "* reprendre les mêmes questions que la question 5 pour mesurer les performances de l'encoder et du decoder en octets/s pour le texte complet tiney_shakespeare.\n",
    "\n",
    "* conclure quant à la comparaison entre les deux encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27180c9-c11e-42e8-9a66-c9c5af3bc511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7618b4-91ba-4b59-a6d6-9db923256d3c",
   "metadata": {},
   "source": [
    "## Question 10. Limites de notre pair encoder\n",
    "\n",
    "* Encoder la chaine \"BUT Informatique de Nevers\" avec le pair encoder précédent.\n",
    "\n",
    "* Proposer et implémenter une solution pour corriger précédent pour le cas où les paires ne sont pas trouvées dans le vocabulaire.\n",
    "\n",
    "* L'encoder modifié devra permettre d'obtenir la même longueur pour le tiny_shakespeare encodé, et proposer une solution pour n'importe quelle chaîne de caractères pour les caractères présents dans le texte original tiny_shakespeare.\n",
    "\n",
    "* Vérifier l'impact de votre modification en termes de performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc4309-1a26-4451-a67a-f641be96d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eaba4c-ebb5-4daf-91fc-2795bad681d4",
   "metadata": {},
   "source": [
    "## Partie 3. Algorithme Byte Pair Encoding (BPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e892ba-c8d2-4910-b10c-8be1e3cba3ea",
   "metadata": {},
   "source": [
    "Dans l'exemple précédent, nous avons construit des paires de lettres de manière irréfléchie, simplement en stockant les paires au fur et à mesure qu'elles se présentent. L'algorithme Byte Pair Encoding permet de construire un vocabulaire de **token** (groupes de 2 ou plus lettres formant le vocabulaire de base) à partir de l'**analyse de la fréquence d'occurrence** dans un **corpus** (dans notre cas, le corpus est le fichier \"tiny shakespeare\"). Byte Pair Encoding (BPE) est un des algorithmes de **tokenization** les plus populaires, utilisé notamment dans les grands modèles de langage type ChatGPT.\n",
    "\n",
    "Nous allons, dans les questions suivantes, implémenter un algorithme BPE à partir de zéro, puis ensuite nous le confronterons à des implémentations industrielles libres (celles d'OpenAI et HuggingFace)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de5fa8-9dc5-4cd0-8396-6c5a1d3fa51c",
   "metadata": {},
   "source": [
    "## Question 11. Fréquence de mots\n",
    "\n",
    "Ecrire une fonction `frequence_mots` qui prend comme paramètre une chaine de caractères `input_str` et qui renvoie un dictionnaire dont les clés sont les mots et les valeurs sont le nombre d'occurrences de ces mots dans la chaîne.\n",
    "\n",
    "Par exemple, dans la chaîne \"le chien Pluto et le chien Milou\", le mot \"chien\" est présent 2 fois, le mot 'le' aussi, on obtiendra donc: \n",
    "\n",
    "{'le': 2, 'chien': 2, 'Pluto': 1, 'et': 1, 'Milou': 1)}\n",
    "\n",
    "Nous allons travailler, dans ce qui suit, avec la chaîne de caractères suivante:\n",
    "```python\n",
    "corpus = \"This is the Hugging Face Course. This chapter is about tokenization. This section shows several tokenizer algorithms. Hopefully, you will be able to understand how they are trained and generate tokens.\"\n",
    "```\n",
    "\n",
    "Vérifier que\n",
    "```python\n",
    "mots_freqs = frequence_mots(corpus)\n",
    "print(mots_freqs)\n",
    "```\n",
    "\n",
    "renvoie bien\n",
    "\n",
    "```python\n",
    "{'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course.': 1, 'chapter': 1, 'about': 1, 'tokenization.': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms.': 1, 'Hopefully,': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens.': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbead76c-e064-4baa-b5d9-5cd49d414822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaaa44c-8d77-4be1-b21e-3e44da6f3e3c",
   "metadata": {},
   "source": [
    "## Question 12. Alphabet et vocabulaire\n",
    "\n",
    "L'étape suivante est de déterminer le vocabulaire de base, formé par l'ensemble des caractères utilisés dans le corpus.\n",
    "\n",
    "* Ecrire une fonction `calcule_alphabet` qui prend comme paramètre un dictionnaire de fréquences de mots `dict_freq` et qui renvoie la liste des lettres utilisées ;\n",
    "\n",
    "* ensuite, créer le `vocabulaire` en ajoutant le token spécial `<|endoftext|>`:\n",
    "```python\n",
    "vocabulaire = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "```\n",
    "\n",
    "Vous vérifierez que vous obtenez le vocabulaire suivant :\n",
    "```python\n",
    "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f03f0c3-3d36-4cab-bd70-4613445fb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ea284-e593-4172-b3ea-fbd7ddaf8565",
   "metadata": {},
   "source": [
    "## Question 13. Splits\n",
    "\n",
    "Ecrire une fonction `calcule_splits` qui prend comme paramètre une liste de mots `liste_mots`et qui renvoie un dictionnaire qui associe à chaque mot la liste des lettres qui le composent. Par exemple :\n",
    "```python\n",
    "{'This': ['T', 'h', 'i', 's'], 'is': ['i', 's'], ...\n",
    "```\n",
    "Appliquer cette fonction aux mots servant de clé dans la dictionnaire `freqs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62272ef6-77d3-4c1a-92d0-988d52c2709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0121c-385d-4933-8243-6a95d8712e24",
   "metadata": {},
   "source": [
    "## Question 14 - Fréquence des paires de lettres ou groupes de lettres\n",
    "\n",
    "Il s'agit ensuite de déterminer la fréquence de l'occurrence de chaque paire de lettres. Par exemple, le mot 'This' est associé aux lettres 'T', 'h', 'i et 's'. Il faut chercher, dans tous les mots, le nombre d'occurrences de 'T', 'h', puis de 'h','i', et de 'i','s'. Et ainsi de suite pour chaque mot.\n",
    "\n",
    "Ecrire une fonction `calcule_pair_freqs` qui retourne un dictionnaire avec comme clé un couple de lettres et comme valeur le nombre d'occurrences trouvées dans tous les mots. \n",
    "\n",
    "On aura par exemple en sortie:\n",
    "\n",
    "```python\n",
    "{('T', 'h'): 3,\n",
    " ('h', 'i'): 3,\n",
    " ('i', 's'): 5,\n",
    " ('t', 'h'): 3,\n",
    " ('h', 'e'): 2,\n",
    " ('H', 'u'): 1,\n",
    " ('u', 'g'): 1,\n",
    " ('g', 'g'): 1,\n",
    " ('g', 'i'): 1,\n",
    " ('i', 'n'): 2,\n",
    " ('n', 'g'): 1,\n",
    " ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffee04-49d5-481e-b10d-46b93870484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3a797-8b5e-47e7-a0c7-8d9451d5301a",
   "metadata": {},
   "source": [
    "## Question 15. Paire la plus fréquente\n",
    "\n",
    "Créer une fonction `paire_la_plus_frequente` qui prend comme paramètre le dictionnaire issu de la fonction précédente (celui contenant la fréquence de chaque paire) et qui retourne la paire la plus fréquente du corpus ainsi que le nombre correspondant à la fréquence. Si deux paires présentent le même nombre d'occurrences, la fonction renvoie la première paire rencontrée dans le parcours de l'ensemble des paires.\n",
    "\n",
    "Vérifier que `paire_la_plus_frequente(pair_freqs)` renvoie `(('i', 's'), 5)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6077c5-8bd5-49a8-a01d-65b8072cea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd64286-6c3c-44b8-9bbf-2e53b74b4f5b",
   "metadata": {},
   "source": [
    "## Question 16. Extension du vocabulaire\n",
    "\n",
    "On va maintenant rajouter au vocabulaire les combinaisons de lettres les plus fréquentes dans le texte.\n",
    "\n",
    "* créer un dictionnaire `fusions` qui associe, à la paire précédente `('i', 's')` la paire concaténée `'is'`\n",
    "\n",
    "* ajouter cette chaîne concaténée à la liste `vocabulaire`.\n",
    "\n",
    "Remarque : cette question est très facile, inutile de créer une fonction.\n",
    "\n",
    "Vous vérifierez que\n",
    "\n",
    "```python\n",
    "print(fusions)\n",
    "print(vocabulaire)\n",
    "```\n",
    "renvoie\n",
    "```\n",
    "{('i', 's'): 'is'}\n",
    "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'is']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b91e46-4718-40df-b4aa-febbb8e5d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb801b-d118-489b-898b-30ba79e73fe3",
   "metadata": {},
   "source": [
    "## Question 16. Fusion des paires\n",
    "\n",
    "C'est la dernière étape de l'algorithme de création du vocabulaire. On se donne une taille maximale du vocabulaire `vocab_size` que l'on fixe à `50`. Reproduire l'étape précédente (reherche de la paire la plus fréquence, fusion, ajout au vocabulaire) jusqu'à ce que la taille maximale du vocabulaire soit atteinte.\n",
    "\n",
    "Pour cette valeur de `vocab_size`, vérifier que vous obtenez le vocabulaire suivant :\n",
    "\n",
    "```python\n",
    "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'is', 'is', 'er', 'to', 'en', 'Th', 'This', 'th', 'ou', 'se', 'tok', 'token', 'nd', 'the', 'in', 'ab', 'tokeni', 'tokeniz', 'at', 'io']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900bfce-772d-41ea-94de-a4ec05cdbdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f54eb-bc03-4e98-bca3-080a5c8fc347",
   "metadata": {},
   "source": [
    "## Question 17 - Tokenization\n",
    "\n",
    "La dernière étape de ce voyage vers les tokens consiste à créer une fonction `tokenize` qui prend une chaine de caractères et, comme dans le début de ce TP, contient l'ensemble des entiers faisant référence au vocabulaire de 50 termes construit précédemment.\n",
    "\n",
    "Vérifier que la tokenization :\n",
    "\n",
    "```python\n",
    "print(tokenize('This is not a token.'))\n",
    "```\n",
    "\n",
    "Donne bien\n",
    "\n",
    "```python\n",
    "['This', 'is', 'n', 'o', 't', 'a', 'token', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2625f-094f-4cf9-bcb4-f5ead7a8f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c50540-7976-483a-93e9-9965fea46600",
   "metadata": {},
   "source": [
    "## Question 18 - Bilan de la qualité algorithmique\n",
    "\n",
    "Pour la châine de caractères `This is not a token`, comparer la taille de la liste d'entiers obtenue pour chacun des 3 tokenizers étudiés.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da73d1-9e8e-4e3e-a09a-9b06c457478c",
   "metadata": {},
   "source": [
    "## Question 19 - Performances de notre BPE\n",
    "\n",
    "Reprendre les questions précédentes en travaillant à partir du corpus `tinyshakespeare`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e647ec-3ad1-4bba-8218-aada2429221c",
   "metadata": {},
   "source": [
    "## Question 20. Implémentations industrielles de tokenizers GPT2 - OpenAI, HuggingFace\n",
    "\n",
    "* BPE est utilisé par OpenAI pour ses gpt depuis gpt2. C'est la bibliothèque `tiktoken` (https://github.com/openai/tiktoken) qui implémente cet algorithme\n",
    "\n",
    "* BPE est aussi utilisé par un autre grand acteur de l'IA générative : HuggingFace, dans sa bibliothèque `transformers` (https://github.com/huggingface/transformers)\n",
    "\n",
    "Pour utiliser la bibliothèque **tiktoken**, encoder/décoder :\n",
    "```python\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(f\"Nombre d'élements dans le vocabulaire : {enc.n_vocab}\")\n",
    "enc.encode('This is not a token')\n",
    "```\n",
    "\n",
    "Pour utiliser la bibliothèque **transformers**, encoder/décoder :\n",
    "```python\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer(\"This is not a token\")['input_ids']\n",
    "```\n",
    "\n",
    "* Quelle est la taille du vocabulaire `gpt2`?\n",
    "\n",
    "* Vérifier que les deux implémentations renvoient la même liste d'entiers pour l'encodage de la chaîne `This is not a token`.\n",
    "\n",
    "* Comparer ces implémentations de BPE avec celle que nous avons faite précédemment.\n",
    "\n",
    "* Comparer ces deux bibliothèques en encodage/décodage par rapport à la vitesse en octets/seconde.\n",
    "\n",
    "* Conclure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801cf09c-ecc5-4d0b-a9fb-3facc42079a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
